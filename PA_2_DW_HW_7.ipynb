{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86BOu4ESYF5f",
    "outputId": "6c3aaf83-3258-456b-f89c-c530b35d404e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dmba in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "!pip install dmba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from dmba import printTermDocumentMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pwsIZHMQdHVi"
   },
   "outputs": [],
   "source": [
    "doc = ['Thanks John!<br /><br /><font size=\"3\"> &quot;Illustrations and demos will be provided for students to work through on their own&quot;</font>. Do we need that to finish project? If yes, where to find the illustration and demos? Thanks for your help.<img title=\"smiles\" alt=\"smiles\" src=\"http://lms.statistics.com/pix/smartpix.php/statistics_com_1/s/smiley.gif\" \\><br /><br />']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8mdopIoTd6GH"
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z8O6ZjINeI-k"
   },
   "outputs": [],
   "source": [
    "counts = count_vect.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIbAfZnleNEc",
    "outputId": "eb810f92-4a7f-4cd4-c754-096b6f9ea349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  S1\n",
      "alt                1\n",
      "and                2\n",
      "be                 1\n",
      "br                 4\n",
      "com                1\n",
      "demos              2\n",
      "do                 1\n",
      "find               1\n",
      "finish             1\n",
      "font               2\n",
      "for                2\n",
      "gif                1\n",
      "help               1\n",
      "http               1\n",
      "if                 1\n",
      "illustration       1\n",
      "illustrations      1\n",
      "img                1\n",
      "john               1\n",
      "lms                1\n",
      "need               1\n",
      "on                 1\n",
      "own                1\n",
      "php                1\n",
      "pix                1\n",
      "project            1\n",
      "provided           1\n",
      "quot               2\n",
      "size               1\n",
      "smartpix           1\n",
      "smiles             2\n",
      "smiley             1\n",
      "src                1\n",
      "statistics         1\n",
      "statistics_com_1   1\n",
      "students           1\n",
      "thanks             2\n",
      "that               1\n",
      "the                1\n",
      "their              1\n",
      "through            1\n",
      "title              1\n",
      "to                 3\n",
      "we                 1\n",
      "where              1\n",
      "will               1\n",
      "work               1\n",
      "yes                1\n",
      "your               1\n"
     ]
    }
   ],
   "source": [
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "580LF1Wwebob"
   },
   "outputs": [],
   "source": [
    "# Q1: some of the non-word tokens include: alt, br, com, gif, http, img, lms, php, pix, quot, smartpix, and src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eig37ii8e5ce"
   },
   "outputs": [],
   "source": [
    "# Q2: Even without knowing the business goal, some tokens can be removed as they are not particularly important to the meaning of the passage.\n",
    "# These include the non-word tokens from the previous question: alt, br, com, gif, http, img, lms, php, pix, quot, smartpix, and src as well as font, size, title, statistics, and statistics_com_1\n",
    "# In addition, stop words such as and, will, be, for, to, through, on, we, that, the, and perhaps a couple others could be removed as well while retaining the general meaning of the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sELoQy4qjhRq"
   },
   "outputs": [],
   "source": [
    "# Q3: Important terms to determine if the post requires the attention of the instructor could include John, Illustrations, demos, provided, students, work, own, need, where, find, finish, project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "pcBvlfrDkZNQ"
   },
   "outputs": [],
   "source": [
    "# Q4: The problem that can arise when using a bag-of-words approach is that the order of words is lost. \n",
    "# For example, it's not clear from the bag-of-words format what the verb finish refers to. Not only could it refer to project, but it could also refer to illustrations and demos as well.\n",
    "# However, for a problem mainly concerned with classification, a BoW model is sufficient to meet the goal rather than needing a meaning-extracting aproach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5rpZsAj52Qf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PA 2 DW HW 7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
